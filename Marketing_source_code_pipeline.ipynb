{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b1e4b9",
   "metadata": {},
   "source": [
    "### Project - Propensify\n",
    "#### Submitted by - Gayathri Keerthivasagam\n",
    "### Description:\n",
    "The end-to-end implementation of the propensity model to identify potential customers is detailed below.  \n",
    "Comprehensive exploratory data analysis (EDA) and determination of the best resampling technique, along with the optimal machine learning model for this marketing campaign, are documented in another notebook titled 'Machine_Learning_Model_Implementation.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df5b612",
   "metadata": {},
   "source": [
    "### Process overview\n",
    "\n",
    "1.Data Collection  \n",
    "2.Data Cleaning and Preprocessing  \n",
    "3.Feature Engineering and Selection  \n",
    "4.Dealing with Imbalanced Data  \n",
    "5.Model Selection  \n",
    "6.Model Training and Evaluation   \n",
    "7.Save the model as pickle file  \n",
    "8.Load the model and predict for the test dataset  \n",
    "9.Append the predicted target  \n",
    "10.Save the final test file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45769b28",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67db1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#libraries for preprocessing steps\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#libraries for resampling the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#libraries for machine learning models\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#libraries for cross validation,train test split,hyperparameter tuning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#libraries for performance metrics\n",
    "from sklearn.metrics import accuracy_score,classification_report,precision_score,recall_score,f1_score,roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,roc_curve,roc_auc_score,RocCurveDisplay, log_loss\n",
    "\n",
    "#libraries to regulate warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa282b6d",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2093dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('train.xlsx')\n",
    "test_df = pd.read_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c7211",
   "metadata": {},
   "source": [
    "### Drop unnecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbaa25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignoring the additional columns available other than the columns mentioned in the project decription.\n",
    "#Dropping the 'profit' and 'id' features.\n",
    "def train_exclude_features(train_df):\n",
    "    train_df.drop(['profit','id'],axis=1,inplace=True)\n",
    "#Delete the last two rows as it has only null values\n",
    "    train_df= train_df.iloc[:-2]\n",
    "    return train_df\n",
    "def test_exclude_features(test_df):\n",
    "    test_df.drop(['id'],axis=1,inplace=True)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f46c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = train_exclude_features(train_df)\n",
    "new_test_df = test_exclude_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c35d8",
   "metadata": {},
   "source": [
    "Thus the additional features such as 'profit' and 'id' are dropped as mentioned in the project description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc0f8a",
   "metadata": {},
   "source": [
    "### Null value Treatment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aebc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_value_treatment(dataset):\n",
    "# Calculate mean age for each profession\n",
    "    mean_age_by_profession = dataset.groupby('profession')['custAge'].mean()\n",
    "# Impute null values in 'custAge' based on mean age for each profession\n",
    "    for profession,mean_age in mean_age_by_profession.items():\n",
    "        dataset.loc[(dataset['profession'] == profession) & (dataset['custAge'].isnull()), 'custAge'] = mean_age\n",
    "#Mode imputation for categorical column 'day_of_week'\n",
    "    missing_cat_column = ['day_of_week']\n",
    "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    dataset[missing_cat_column] = mode_imputer.fit_transform(dataset[missing_cat_column])\n",
    "#Impute null values in 'schooling' column with mode value based on 'profession'.\n",
    "    schooling_by_profession = dataset.groupby('profession')['schooling'].agg(lambda x: x.mode())\n",
    "    for profession, mode_value in schooling_by_profession.items():\n",
    "        dataset.loc[(dataset['profession'] == profession) & (dataset['schooling'].isnull()), 'schooling'] = mode_value\n",
    "    return dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac4a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = null_value_treatment(new_train_df)\n",
    "new_test_df = null_value_treatment(new_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d7ca3",
   "metadata": {},
   "source": [
    "Thus the null values in the 'custAge','day_of_week','schooling' features are imputed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077de547",
   "metadata": {},
   "source": [
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13ba0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dataset):\n",
    "    #checking for duplicates\n",
    "    num_duplicates = dataset.duplicated().sum()\n",
    "    print(num_duplicates)\n",
    "    #Removing the duplicate records\n",
    "    dataset.drop_duplicates(inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27bd0f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "new_train_df = remove_duplicates(new_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fc632",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5225d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the target variable for class 0 and class 1\n",
    "new_train_df['responded'] = new_train_df['responded'].map(lambda x: 0 if x == 'no' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa9a7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(dataset):\n",
    "    dataset.drop('pmonths',axis=1,inplace=True)\n",
    "#Define conditions and choices for pdays\n",
    "    conditions = [\n",
    "        (dataset['pdays'] == 999),\n",
    "        (dataset['pdays'] < 5),\n",
    "        ((dataset['pdays'] >= 5) & (dataset['pdays'] <= 10)),\n",
    "        ((dataset['pdays'] > 10) & (dataset['pdays'] != 999)) ]\n",
    "    choices = ['not_contacted', 'less_than_5_days', '5_to_10 days', 'greater_than_10_days']\n",
    "    # Create the 'pdays' column based on conditions\n",
    "    dataset['pdays'] = np.select(conditions, choices, default='unknown')\n",
    "    \n",
    "    \n",
    "# Define conditions and choices for pastEmail\n",
    "    conditions_pastEmail = [\n",
    "        (dataset['pastEmail'] == 0),\n",
    "        (dataset['pastEmail'] < 10),\n",
    "        (dataset['pastEmail'] >= 10) ]\n",
    "    choices_pastEmail = ['no_email_sent', 'less_than_10', 'more_than_10']\n",
    "    # Create the 'pastEmail_category' column based on conditions\n",
    "    dataset['pastEmail'] = np.select(conditions_pastEmail, choices_pastEmail, default='unknown')\n",
    "    \n",
    "\n",
    "# Define conditions and choices for 'custAge'\n",
    "    conditions_custAge = [\n",
    "         (dataset['custAge'] <= 30),\n",
    "         ((dataset['custAge'] > 30) & (dataset['custAge'] <= 45)),\n",
    "         ((dataset['custAge'] > 45) & (dataset['custAge'] <= 60)),\n",
    "         ((dataset['custAge'] > 60) & (dataset['custAge'] <= 75)),\n",
    "         (dataset['custAge'] > 75) ]\n",
    "    choices_custAge = ['below_30', '30-45', '45-60','60-75','above_75']\n",
    "    # Create the 'custAge' column based on conditions\n",
    "    dataset['custAge'] = np.select(conditions_custAge, choices_custAge, default='unknown')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17bd7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = feature_engineering(new_train_df)\n",
    "new_test_df = feature_engineering(new_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d6518",
   "metadata": {},
   "source": [
    "Feature engineering is done for 'custAge','pdays','pmonths' and 'pastEmail' features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4fb6f",
   "metadata": {},
   "source": [
    "### Identifying numerical,categorical and binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b510185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the numerical features in the dataset\n",
    "numerical_columns = new_train_df._get_numeric_data().columns\n",
    "#Find the categorical features in the dataset\n",
    "categorical_columns = new_train_df.drop(numerical_columns,axis=1).columns\n",
    "#identify the binary columns\n",
    "binary_cols = []\n",
    "for i in new_train_df.select_dtypes(include=['int', 'float']).columns:\n",
    "    unique_values = new_train_df[i].unique()\n",
    "    if np.in1d(unique_values, [0, 1]).all():\n",
    "        binary_cols.append(i)\n",
    "numerical_columns = [i for i in numerical_columns if i not in binary_cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d9279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the skewness of the numerical features\n",
    "\n",
    "def skewness(dataset, numerical_columns):\n",
    "    # Calculate skewness of each column\n",
    "    skewness = dataset[numerical_columns].skew()\n",
    "    \n",
    "    # Find columns with positive skewness\n",
    "    positive_skew_cols = skewness[skewness > 1].index.tolist()\n",
    "    print(positive_skew_cols)\n",
    "    \n",
    "    # Apply log transformation to columns with positive skewness\n",
    "    for col in positive_skew_cols:\n",
    "        dataset[col] = np.log1p(dataset[col])\n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "079b97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['campaign', 'previous']\n",
      "['campaign', 'previous']\n"
     ]
    }
   ],
   "source": [
    "new_train_df = skewness(new_train_df,numerical_columns)\n",
    "new_test_df = skewness(new_test_df,numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4f684",
   "metadata": {},
   "source": [
    "### Feature Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf0b2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_encoding(dataset):\n",
    "# Initialize LabelEncoder    \n",
    "    label_encoder = LabelEncoder()\n",
    "#specify the features that needs to be label encoded\n",
    "    cat_cols1 = ['profession','schooling', 'month', 'day_of_week']\n",
    "    for col in cat_cols1:\n",
    "        dataset[col] = label_encoder.fit_transform(dataset[col])\n",
    "#specify the features that needs to be one hot encoded        \n",
    "    cat_cols2 = ['custAge','marital', 'default', 'housing', 'loan','contact', 'poutcome','pdays','pastEmail']\n",
    "# Use pd.get_dummies() to one-hot encode the categorical columns\n",
    "    encoded_features = pd.get_dummies(dataset[cat_cols2])\n",
    "# Concatenate the original DataFrame with the encoded features along the columns axis\n",
    "    dataset = pd.concat([dataset, encoded_features], axis=1)\n",
    "# Drop the original categorical columns \n",
    "    dataset.drop(cat_cols2, axis=1, inplace=True) \n",
    "\n",
    "    return dataset\n",
    "\n",
    "new_train_df = feature_encoding(new_train_df)\n",
    "new_test_df = feature_encoding(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4483b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(dataset, numerical_columns):\n",
    "    sc_x = StandardScaler()\n",
    "    dataset[numerical_columns] = sc_x.fit_transform(dataset[numerical_columns])\n",
    "    return dataset\n",
    "\n",
    "new_train_df = feature_scaling(new_train_df, numerical_columns)\n",
    "new_test_df = feature_scaling(new_test_df, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e17e8",
   "metadata": {},
   "source": [
    "### Split the dataset into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74a5d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the features and target\n",
    "X = new_train_df.drop('responded',axis= 1)\n",
    "y = new_train_df['responded']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10bf5c",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab27887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation with SMOTE technique\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=24)\n",
    "smote = SMOTE(random_state=12)\n",
    "gb_classifier = GradientBoostingClassifier(random_state=12)\n",
    "pipeline = Pipeline([('smote', smote), ('gb_classifier', gb_classifier)])\n",
    "\n",
    "# Define the parameter grid to search\n",
    "parameters = {\n",
    "    'gb_classifier__n_estimators': [100, 200, 300],\n",
    "    'gb_classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gb_classifier__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           param_grid = parameters, \n",
    "                           cv=skf, \n",
    "                           scoring='f1', \n",
    "                           n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "best_est = grid_search.best_estimator_\n",
    "best_model = grid_search.best_estimator_['gb_classifier']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac6f38",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d728a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6409759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069906a0",
   "metadata": {},
   "source": [
    "### Predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15c736f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset\n",
    "predictions = loaded_model.predict(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8b8e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33074ae",
   "metadata": {},
   "source": [
    "### Predictions are added to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b0f1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the test dataset\n",
    "test_predictions['responded'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce6cfd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "responded\n",
       "0    23347\n",
       "1     9603\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions['responded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4855e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the target 'yes' for 1 and 'no' for 0\n",
    "test_predictions['responded'] = test_predictions['responded'].replace({0: 'no', 1: 'yes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "625d0708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custAge</th>\n",
       "      <th>profession</th>\n",
       "      <th>marital</th>\n",
       "      <th>schooling</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>pastEmail</th>\n",
       "      <th>responded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30-45</td>\n",
       "      <td>0</td>\n",
       "      <td>married</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>not_contacted</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.199</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>0.886</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>less_than_10</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30-45</td>\n",
       "      <td>7</td>\n",
       "      <td>married</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>less_than_5_days</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>success</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>92.379</td>\n",
       "      <td>-29.8</td>\n",
       "      <td>0.788</td>\n",
       "      <td>5017.5</td>\n",
       "      <td>less_than_10</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45-60</td>\n",
       "      <td>1</td>\n",
       "      <td>married</td>\n",
       "      <td>5</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>not_contacted</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>92.893</td>\n",
       "      <td>-46.2</td>\n",
       "      <td>1.327</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>less_than_10</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>below_30</td>\n",
       "      <td>0</td>\n",
       "      <td>single</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>not_contacted</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.444</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4.964</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no_email_sent</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30-45</td>\n",
       "      <td>7</td>\n",
       "      <td>divorced</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>not_contacted</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>93.200</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.153</td>\n",
       "      <td>5195.8</td>\n",
       "      <td>no_email_sent</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    custAge  profession   marital  schooling  default housing loan   contact  \\\n",
       "0     30-45           0   married          6       no      no  yes  cellular   \n",
       "1     30-45           7   married          3       no      no   no  cellular   \n",
       "2     45-60           1   married          5  unknown     yes   no  cellular   \n",
       "3  below_30           0    single          6       no      no   no  cellular   \n",
       "4     30-45           7  divorced          3       no     yes   no  cellular   \n",
       "\n",
       "   month  day_of_week  ...             pdays  previous     poutcome  \\\n",
       "0      9            4  ...     not_contacted  0.693147      failure   \n",
       "1      9            3  ...  less_than_5_days  0.693147      success   \n",
       "2      6            2  ...     not_contacted  0.693147      failure   \n",
       "3      1            4  ...     not_contacted  0.000000  nonexistent   \n",
       "4      7            3  ...     not_contacted  0.000000  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed  \\\n",
       "0         -1.1          94.199          -37.5      0.886       4963.6   \n",
       "1         -3.4          92.379          -29.8      0.788       5017.5   \n",
       "2         -1.8          92.893          -46.2      1.327       5099.1   \n",
       "3          1.4          93.444          -36.1      4.964       5228.1   \n",
       "4         -0.1          93.200          -42.0      4.153       5195.8   \n",
       "\n",
       "       pastEmail responded  \n",
       "0   less_than_10       yes  \n",
       "1   less_than_10       yes  \n",
       "2   less_than_10        no  \n",
       "3  no_email_sent        no  \n",
       "4  no_email_sent        no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8800c7",
   "metadata": {},
   "source": [
    "### Save the final test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa8668a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the updated test dataset\n",
    "test_predictions.to_excel('test_predictions.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dcbc4",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    " Utilizing gradient boosting algorithm along with the SMOTE technique, potential customers for the insurance company have been successfully identified with an accuracy of 0.87. This achievement enables the company to target their potential customers effectively, leading to success in their marketing campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0177f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
